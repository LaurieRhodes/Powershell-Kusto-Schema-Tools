// ============================================================================
// Azure Data Explorer KQL Script for HDInsightSparkTaskEvents - UPDATED VERSION
// ============================================================================
// Generated: 2025-09-14 10:15:44
// Table type: Microsoft
// Schema discovered using hybrid approach (Management API + getschema)
// Data type corrections applied: TenantId->guid, Double->real (empirical fixes)
// All columns including underscore columns included
// Original columns: 52, Final columns: 52
// ============================================================================

.create-merge table HDInsightSparkTaskEventsRaw (records:dynamic)

.alter-merge table HDInsightSparkTaskEventsRaw policy retention softdelete = 1d

.alter table HDInsightSparkTaskEventsRaw policy caching hot = 1h

// JSON mapping - choose appropriate option based on data structure
.create-or-alter table HDInsightSparkTaskEventsRaw ingestion json mapping 'HDInsightSparkTaskEventsRawMapping' '[{"column":"records","Properties":{"path":"$.records"}}]'
// Alternative for direct events: '[{"column":"records","Properties":{"path":"$"}}]'

.create-merge table HDInsightSparkTaskEvents(
TimeGenerated:datetime,
TenantId:guid,
PeakExecutionMemory:long,
SchedulerDelay:long,
RecordsRead:long,
BytesRead:long,
RecordsWritten:long,
BytesWritten:long,
ShuffleFetchWaitTime:long,
ShuffleTotalBytesRead:long,
ShuffleTotalBlocksFetched:long,
ShuffleLocalBlocksFetched:long,
ShuffleRemoteBlocksFetched:long,
ShuffleWriteTime:long,
ShuffleBytesWritten:long,
ShuffleRecordsWritten:long,
NumUpdatedBlockStatuses:long,
ClusterTenantId:string,
Role:string,
Host:string,
ClusterDnsName:string,
Region:string,
IpAddress:string,
UpdatedBlocks:long,
UserSubscriptionId:string,
OutputMetrics:long,
ShuffleWriteMetrics:long,
ApplicationId:string,
StageId:string,
TaskId:string,
AttemptId:string,
ExecutorId:string,
LaunchTime:datetime,
FinishTime:datetime,
Failed:bool,
Killed:bool,
EndReason:string,
TaskType:string,
DiskBytesSpilled:long,
ExecutorCPUTime:long,
ExecutorDeserializeCPUTime:long,
ExecutorDeserializeTime:long,
ExecutorRunTime:long,
JvmGcTime:long,
MemoryBytesSpilled:long,
ResultSerializationTime:long,
ResultSize:long,
ShuffleReadMetrics:long,
InputMetrics:long,
SourceSystem:string,
Type:string,
_ResourceId:string,
_TimeReceived:datetime)

.alter table HDInsightSparkTaskEvents policy caching hot = 1d

.create-or-alter function HDInsightSparkTaskEventsExpand() {
HDInsightSparkTaskEventsRaw
| mv-expand events = records
// Alternative for non-nested: | extend events = records
| project
TimeGenerated=todatetime(events.TimeGenerated),
TenantId=toguid(events.TenantId),
PeakExecutionMemory=tolong(events.PeakExecutionMemory),
SchedulerDelay=tolong(events.SchedulerDelay),
RecordsRead=tolong(events.RecordsRead),
BytesRead=tolong(events.BytesRead),
RecordsWritten=tolong(events.RecordsWritten),
BytesWritten=tolong(events.BytesWritten),
ShuffleFetchWaitTime=tolong(events.ShuffleFetchWaitTime),
ShuffleTotalBytesRead=tolong(events.ShuffleTotalBytesRead),
ShuffleTotalBlocksFetched=tolong(events.ShuffleTotalBlocksFetched),
ShuffleLocalBlocksFetched=tolong(events.ShuffleLocalBlocksFetched),
ShuffleRemoteBlocksFetched=tolong(events.ShuffleRemoteBlocksFetched),
ShuffleWriteTime=tolong(events.ShuffleWriteTime),
ShuffleBytesWritten=tolong(events.ShuffleBytesWritten),
ShuffleRecordsWritten=tolong(events.ShuffleRecordsWritten),
NumUpdatedBlockStatuses=tolong(events.NumUpdatedBlockStatuses),
ClusterTenantId=tostring(events.ClusterTenantId),
Role=tostring(events.Role),
Host=tostring(events.Host),
ClusterDnsName=tostring(events.ClusterDnsName),
Region=tostring(events.Region),
IpAddress=tostring(events.IpAddress),
UpdatedBlocks=tolong(events.UpdatedBlocks),
UserSubscriptionId=tostring(events.UserSubscriptionId),
OutputMetrics=tolong(events.OutputMetrics),
ShuffleWriteMetrics=tolong(events.ShuffleWriteMetrics),
ApplicationId=tostring(events.ApplicationId),
StageId=tostring(events.StageId),
TaskId=tostring(events.TaskId),
AttemptId=tostring(events.AttemptId),
ExecutorId=tostring(events.ExecutorId),
LaunchTime=todatetime(events.LaunchTime),
FinishTime=todatetime(events.FinishTime),
Failed=tobool(events.Failed),
Killed=tobool(events.Killed),
EndReason=tostring(events.EndReason),
TaskType=tostring(events.TaskType),
DiskBytesSpilled=tolong(events.DiskBytesSpilled),
ExecutorCPUTime=tolong(events.ExecutorCPUTime),
ExecutorDeserializeCPUTime=tolong(events.ExecutorDeserializeCPUTime),
ExecutorDeserializeTime=tolong(events.ExecutorDeserializeTime),
ExecutorRunTime=tolong(events.ExecutorRunTime),
JvmGcTime=tolong(events.JvmGcTime),
MemoryBytesSpilled=tolong(events.MemoryBytesSpilled),
ResultSerializationTime=tolong(events.ResultSerializationTime),
ResultSize=tolong(events.ResultSize),
ShuffleReadMetrics=tolong(events.ShuffleReadMetrics),
InputMetrics=tolong(events.InputMetrics),
SourceSystem=tostring(events.SourceSystem),
Type=tostring(events.Type),
_ResourceId=tostring(events._ResourceId),
_TimeReceived=todatetime(now())
}

.alter table HDInsightSparkTaskEvents policy update @'[{"Source": "HDInsightSparkTaskEventsRaw", "Query": "HDInsightSparkTaskEventsExpand()", "IsEnabled": "True", "IsTransactional": true}]'
